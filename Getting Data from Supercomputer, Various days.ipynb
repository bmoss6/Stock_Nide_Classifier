{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import paramiko\n",
    "\n",
    "from statistic import Statistic\n",
    "from statistic import *\n",
    "from FslAccessor import FslAccessor\n",
    "\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard values in a sliding window\n",
    "def get_mean_in_steps(curve, interval_size, step):\n",
    "    \"\"\" Takes the mean on a sliding window of the curve.\n",
    "    :param curve: a 1d signal (numpy array of real numbers)\n",
    "    :param interval_size: the size of the sliding window (positive integer)\n",
    "    :param step: the step size in the sliding window (positive integer)\n",
    "    :returns: a list which contains the mean of each segment\n",
    "    \"\"\"\n",
    "    len_curve = len(curve)\n",
    "    start_index = 0\n",
    "    out = []\n",
    "    while start_index + interval_size < len_curve:\n",
    "        signal = curve[start_index: start_index + interval_size]\n",
    "        mean = np.mean(signal)\n",
    "        out.append(mean)\n",
    "        start_index += step\n",
    "    return out\n",
    "\n",
    "def get_var_in_steps(curve, interval_size, step):\n",
    "    \"\"\" Takes the variance on a sliding window of the curve.\n",
    "    :param curve: a 1d signal (numpy array of real numbers)\n",
    "    :param interval_size: the size of the sliding window (positive integer)\n",
    "    :param step: the step size in the sliding window (positive integer)\n",
    "    :returns: a list which contains the variance of each segment\n",
    "    \"\"\"\n",
    "    len_curve = len(curve)\n",
    "    start_index = 0\n",
    "    out = []\n",
    "    while start_index + interval_size < len_curve:\n",
    "        signal = curve[start_index: start_index + interval_size]\n",
    "        var = np.var(signal)\n",
    "        out.append(var)\n",
    "        start_index += step\n",
    "    return out\n",
    "\n",
    "# Now we get some spectral sliding window measurements\n",
    "def fourier_over_time_overlap_log(curve, interval_size, step):\n",
    "    \"\"\" takes a curve does a fourier transform on a sliding window of the curve.\n",
    "    :param curve: a 1d signal (numpy array of real numbers)\n",
    "    :param interval_size: the size of the sliding window (positive integer)\n",
    "    :param step: the step size in the sliding window (positive integer)\n",
    "    :returns: a list length num_splits which contains the fourier signal of each segment\n",
    "    \"\"\"\n",
    "    len_curve = len(curve)\n",
    "    start_index = 0\n",
    "    out = []\n",
    "    while start_index + interval_size < len_curve:\n",
    "        signal = curve[start_index: start_index + interval_size]\n",
    "        freq = np.log(np.fft.fftshift(np.abs(np.fft.fft(signal)))) \n",
    "        #freq = np.fft.fftshift(np.abs(np.fft.fft(signal)))\n",
    "        out.append(freq)\n",
    "        start_index += step\n",
    "    return out\n",
    "\n",
    "# a single measure (weighted mean) from the spectral data\n",
    "def mean_log_frequency(my_signals):\n",
    "    \"\"\"turns a list of log fourier signals into a list of the same size of the mean of those signals,\n",
    "    weighted by their index (to indicate their frequency) - This is one possible feature we can extract from these signals.\n",
    "    :param my_signals: a list which contains the fourier signals\n",
    "    :returns: a list of the same size with the weighted means\n",
    "    \"\"\"\n",
    "    n = len(my_signals)\n",
    "    weights = np.linspace(0, 0.5, len(my_signals[0]))\n",
    "    out = []\n",
    "    for signal in my_signals:\n",
    "        weighted_signal = np.multiply(signal, weights)\n",
    "        out.append(np.mean(weighted_signal))\n",
    "    return out\n",
    "\n",
    "# binning the spectral data and getting the mean and variance\n",
    "def binned_fourier_means(my_signals, n):\n",
    "    \"\"\"\n",
    "    Takes a list of fourier signals into a list of n lists of means, where each list is a single bin from the,\n",
    "    original signals.\n",
    "    :param my_signals: a list which contains the fourier signals\n",
    "    :param n: int, the number of bins\n",
    "    returns: a list of the same length as my_signals whith each entry in the list being a list of length n \n",
    "            (each entry of this list being the bin means)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for signal in my_signals:\n",
    "        length = len(singal) / n\n",
    "        index = 0\n",
    "        bin_vals = []\n",
    "        for i in range(n):\n",
    "            average_val = np.mean(signal(index:index+length))\n",
    "            bin_vals.append(average_val)\n",
    "            index += length\n",
    "        out.append(bin_vals)\n",
    "    return out\n",
    "\n",
    "def binned_fourier_vars(my_signals, n):\n",
    "    \"\"\"\n",
    "    Takes a list of fourier signals into a list of n lists of variances, where each list is a single bin from the,\n",
    "    original signals.\n",
    "    :param my_signals: a list which contains the fourier signals\n",
    "    :param n: int, the number of bins\n",
    "    returns: a list of the same length as my_signals whith each entry in the list being a list of length n \n",
    "            (each entry of this list being the bin variances)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for signal in my_signals:\n",
    "        length = len(singal) / n\n",
    "        index = 0\n",
    "        bin_vals = []\n",
    "        for i in range(n):\n",
    "            var = np.var(signal(index:index+length))\n",
    "            bin_vals.append(var)\n",
    "            index += length\n",
    "        out.append(bin_vals)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Lets get sliding window values for our three types of data measurements: our options are to\n",
    " 1. sample from each window, \n",
    " 2. use the first/last/middle value of each window or \n",
    " 3. use the mean of each window.\n",
    " \n",
    "Based on the results of the cell below it looks like they all typically have a small coefficient of variation (ratio of std deviation to the mean) so I think that taking the mean in these windows should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day = \"110617\"\n",
    "a_day = \"fsl_groups/fslg_market_robustness/compute/NASDAQ/ProcessedData/{0}/\".format(day)\n",
    "csv_ptn = a_day + '{0}_*.csv.gz'.format(day)\n",
    "\n",
    "fa = FslAccessor()\n",
    "#ls = fa.connect_apply(fa.lst_files, path=csv_ptn)\n",
    "#print(ls)\n",
    "lst = fa.pull_obaos(path=csv_ptn, f_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flux_low = np.zeros(int((16 - 9.5) * 3600 / 0.01))\n",
    "\n",
    "for stock in tqdm(lst):\n",
    "    if stock is not None:\n",
    "        stat = Statistic(stock)\n",
    "        dd = stat.delta_depth_flux_mat(d_t=0.01)\n",
    "        time, trace = dd[:, 0], dd[:, 1]\n",
    "        flux_low += trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(flux_low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"delta_depth_flux_{0}_low.npy\".format(day), flux_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flux_d = np.zeros(int((16 - 9.5) * 3600 / 0.01))\n",
    "\n",
    "for stock in tqdm(lst):\n",
    "    if stock is not None:\n",
    "        stat = Statistic(stock)\n",
    "\n",
    "        dd = stat.depth_flux_mat(d_t=0.01)\n",
    "        time, trace = dd[:, 0], dd[:, 1]\n",
    "        flux_d += trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(flux_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"depth_flux_{0}_low.npy\".format(day), flux_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flux_m = np.zeros(int((16 - 9.5) * 3600 / 0.01))\n",
    "\n",
    "for stock in tqdm(lst):\n",
    "    if stock is not None:\n",
    "        stat = Statistic(stock)\n",
    "\n",
    "        dd = stat.message_flux_mat(d_t=0.01)\n",
    "        time, trace = dd[:, 0], dd[:, 1]\n",
    "        flux_m += trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(flux_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"message_flux_{0}_low.npy\".format(day), flux_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start over from what we saved\n",
    "import numpy as np\n",
    "\n",
    "flux_low = np.load(\"delta_depth_flux_{0}_low.npy\".format(day))\n",
    "flux_d = np.load(\"depth_flux_{0}_low.npy\".format(day))\n",
    "flux_m = np.load(\"message_flux_{0}_low.npy\".format(day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_log_freq_delta_depth = mean_log_frequency(fourier_over_time_overlap_log(flux_low, 360000, 1000))\n",
    "# If needed we could go up to steps as small as 100 with this particular data set, it takes a bit but it will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"mlf_delta_depth_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_delta_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_log_freq_depth = mean_log_frequency(fourier_over_time_overlap_log(flux_d, 360000, 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"mlf_depth_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_log_freq_message = mean_log_frequency(fourier_over_time_overlap_log(flux_m, 360000, 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.save(\"mlf_message_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flux_dd_steps = get_mean_in_steps(flux_low, 360000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"avg_delta_depth_{0}_low_hour_window_step1000.npy\".format(day), flux_dd_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flux_d_steps = get_mean_in_steps(flux_d, 360000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save(\"avg_depth_{0}_low_hour_window_step1000.npy\".format(day), flux_d_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flux_m_steps = get_mean_in_steps(flux_m, 360000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"avg_message_{0}_low_hour_window_step1000.npy\".format(day), flux_m_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get even more features!!!\n",
    "days = ['101416', '102116', '102816', '111917', '112617', '120317', '110617', '103117', '111317', '062516', '071016', '091316', '110816', '022817', '042617', '110617', '072517', '121217']\n",
    "print(len(days), type(days[6]))\n",
    "stat.ask_flux_mat(dt=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The day is  110617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2462a0da12422bb7ba756f4b144cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV...\n",
      "Finished Reading CSV in 3.7978391647338867s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.891468048095703s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5459561347961426s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.782581090927124s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.3932769298553467s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.4076311588287354s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.7368531227111816s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5933139324188232s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception: Error reading SSH protocol banner\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 2138, in _check_banner\n",
      "    buf = self.packetizer.readline(timeout)\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/packet.py\", line 367, in readline\n",
      "    buf += self._read_timeout(timeout)\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/packet.py\", line 576, in _read_timeout\n",
      "    raise socket.timeout()\n",
      "socket.timeout\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 1966, in run\n",
      "    self._check_banner()\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 2143, in _check_banner\n",
      "    \"Error reading SSH protocol banner\" + str(e)\n",
      "paramiko.ssh_exception.SSHException: Error reading SSH protocol banner\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading SSH protocol banner\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5240259170532227s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5288658142089844s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5180141925811768s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.682436943054199s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6844491958618164s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5617079734802246s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 4.085468053817749s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.8371870517730713s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.7050578594207764s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception: Error reading SSH protocol banner\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 2138, in _check_banner\n",
      "    buf = self.packetizer.readline(timeout)\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/packet.py\", line 367, in readline\n",
      "    buf += self._read_timeout(timeout)\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/packet.py\", line 576, in _read_timeout\n",
      "    raise socket.timeout()\n",
      "socket.timeout\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 1966, in run\n",
      "    self._check_banner()\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 2143, in _check_banner\n",
      "    \"Error reading SSH protocol banner\" + str(e)\n",
      "paramiko.ssh_exception.SSHException: Error reading SSH protocol banner\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading SSH protocol banner\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517b011609ee45deb3ac47798855b598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2340000,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f69b6b89b1f41ddba0d9896d9d61693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2340000,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1f6075709f43b6b28727463855d78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2340000,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f654dc688ad648c59428d0f548f24b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2994bc063894491ebdb9e2c670fef5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Charles/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/Users/Charles/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The day is  072517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201bfa8ff3e14a7794de49087089dae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6616082191467285s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.85624098777771s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5408358573913574s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6452252864837646s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.3922410011291504s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.3337371349334717s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 4.582437753677368s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5339338779449463s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5716729164123535s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.579061985015869s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.710576295852661s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.561954975128174s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.605067014694214s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.8046700954437256s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.349731922149658s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6690919399261475s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6553049087524414s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception: Error reading SSH protocol banner\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 2138, in _check_banner\n",
      "    buf = self.packetizer.readline(timeout)\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/packet.py\", line 367, in readline\n",
      "    buf += self._read_timeout(timeout)\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/packet.py\", line 576, in _read_timeout\n",
      "    raise socket.timeout()\n",
      "socket.timeout\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 1966, in run\n",
      "    self._check_banner()\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 2143, in _check_banner\n",
      "    \"Error reading SSH protocol banner\" + str(e)\n",
      "paramiko.ssh_exception.SSHException: Error reading SSH protocol banner\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading SSH protocol banner\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.346498966217041s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.634530782699585s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6337931156158447s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.498530864715576s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.7041969299316406s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.780586004257202s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6553401947021484s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.3434927463531494s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 4.743271827697754s!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609fcad3ea034a18abb9228de1a529cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2340000,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6008f7ba3205420aad022f04079522a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2340000,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2a9c3b06554a4eb69779e00ed6f1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2340000,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f5be36b54242fb8e6b35d1a32dd4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25e8e9a93644d74aca1d4add78d4b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The day is  121217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488ce89480b841a494b7cda445d67dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6583099365234375s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.8337080478668213s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5006070137023926s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6585278511047363s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.4634649753570557s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.395301103591919s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.671048879623413s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.549057722091675s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.549743890762329s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.9875099658966064s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6548023223876953s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.502640962600708s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.512917995452881s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.880220890045166s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.3661019802093506s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6669881343841553s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception: Error reading SSH protocol banner\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 2138, in _check_banner\n",
      "    buf = self.packetizer.readline(timeout)\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/packet.py\", line 367, in readline\n",
      "    buf += self._read_timeout(timeout)\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/packet.py\", line 576, in _read_timeout\n",
      "    raise socket.timeout()\n",
      "socket.timeout\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 1966, in run\n",
      "    self._check_banner()\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 2143, in _check_banner\n",
      "    \"Error reading SSH protocol banner\" + str(e)\n",
      "paramiko.ssh_exception.SSHException: Error reading SSH protocol banner\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading SSH protocol banner\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.6860220432281494s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 5.353327035903931s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.5284979343414307s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.3885350227355957s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.704927682876587s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.692498207092285s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.576421022415161s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.667100191116333s!\n",
      "Reading CSV...\n",
      "Finished Reading CSV in 3.7024457454681396s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception: Error reading SSH protocol banner\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 2138, in _check_banner\n",
      "    buf = self.packetizer.readline(timeout)\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/packet.py\", line 367, in readline\n",
      "    buf += self._read_timeout(timeout)\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/packet.py\", line 576, in _read_timeout\n",
      "    raise socket.timeout()\n",
      "socket.timeout\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 1966, in run\n",
      "    self._check_banner()\n",
      "  File \"/Users/Charles/anaconda3/lib/python3.6/site-packages/paramiko/transport.py\", line 2143, in _check_banner\n",
      "    \"Error reading SSH protocol banner\" + str(e)\n",
      "paramiko.ssh_exception.SSHException: Error reading SSH protocol banner\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading SSH protocol banner\n",
      "[Errno 60] Operation timed out\n",
      "[Errno 60] Operation timed out\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe8806a8b45474f889abfa5815fa718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2340000,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cd61beadda4d63b8c892df64b12dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2340000,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c628f590c7cb47e4b9358155986c7996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2340000,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e92848b58043cd96647b2879548c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572697f15d044100b559aa1a2f30bcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "days = ['110617', '072517', '121217']\n",
    "\n",
    "for day in days:\n",
    "    print(\"The day is \", day)\n",
    "    a_day = \"fsl_groups/fslg_market_robustness/compute/NASDAQ/ProcessedData/{0}/\".format(day)\n",
    "    csv_ptn = a_day + '{0}_*.csv.gz'.format(day)\n",
    "\n",
    "    fa = FslAccessor()\n",
    "    lst = fa.pull_obaos(path=csv_ptn, f_num=50)\n",
    "\n",
    "    flux_low = np.zeros(int((16 - 9.5) * 3600 / 0.01))\n",
    "\n",
    "    for stock in tqdm(lst):\n",
    "        if stock is not None:\n",
    "            stat = Statistic(stock)\n",
    "            dd = stat.delta_depth_flux_mat(d_t=0.01)\n",
    "            time, trace = dd[:, 0], dd[:, 1]\n",
    "            flux_low += trace\n",
    "\n",
    "    print(np.shape(flux_low))\n",
    "\n",
    "    np.save(\"delta_depth_flux_{0}_low.npy\".format(day), flux_low)\n",
    "\n",
    "    flux_d = np.zeros(int((16 - 9.5) * 3600 / 0.01))\n",
    "\n",
    "    for stock in tqdm(lst):\n",
    "        if stock is not None:\n",
    "            stat = Statistic(stock)\n",
    "\n",
    "            dd = stat.depth_flux_mat(d_t=0.01)\n",
    "            time, trace = dd[:, 0], dd[:, 1]\n",
    "            flux_d += trace\n",
    "\n",
    "    print(np.shape(flux_d))\n",
    "\n",
    "    np.save(\"depth_flux_{0}_low.npy\".format(day), flux_d)\n",
    "\n",
    "    flux_m = np.zeros(int((16 - 9.5) * 3600 / 0.01))\n",
    "\n",
    "    for stock in tqdm(lst):\n",
    "        if stock is not None:\n",
    "            stat = Statistic(stock)\n",
    "\n",
    "            dd = stat.message_flux_mat(d_t=0.01)\n",
    "            time, trace = dd[:, 0], dd[:, 1]\n",
    "            flux_m += trace\n",
    "\n",
    "    print(np.shape(flux_m))\n",
    "\n",
    "    np.save(\"message_flux_{0}_low.npy\".format(day), flux_m)\n",
    "\n",
    "    # Now lets get the bid and the ask (aggragated, I think... \n",
    "    #   but IDK I just modified Humphrey's code without totally understanding it.)\n",
    "\n",
    "    bid_low = np.zeros(int((16 - 9.5) * 3600 / 0.01))\n",
    "\n",
    "    for stock in tqdm(lst):\n",
    "        stat = Statistic(stock)\n",
    "        if stat.obao is not None:\n",
    "            dd = stat.bid_flux_mat(d_t=0.01)\n",
    "            time, trace = dd[:, 0], dd[:, 1]\n",
    "            bid_low += trace\n",
    "\n",
    "\n",
    "    np.save(\"bid_{0}_low.npy\".format(day), bid_low)\n",
    "\n",
    "    ask_low = np.zeros(int((16 - 9.5) * 3600 / 0.01))\n",
    "\n",
    "    for stock in tqdm(lst):\n",
    "        stat = Statistic(stock)\n",
    "        if stat.obao is not None:\n",
    "            dd = stat.ask_flux_mat(d_t=0.01)\n",
    "            time, trace = dd[:, 0], dd[:, 1]\n",
    "            ask_low += trace\n",
    "\n",
    "\n",
    "    np.save(\"ask_{0}_low.npy\".format(day), ask_low)\n",
    "\n",
    "    # start over from what we saved\n",
    "    import numpy as np\n",
    "\n",
    "    flux_low = np.load(\"delta_depth_flux_{0}_low.npy\".format(day))\n",
    "    flux_d = np.load(\"depth_flux_{0}_low.npy\".format(day))\n",
    "    flux_m = np.load(\"message_flux_{0}_low.npy\".format(day))\n",
    "    ask_low = np.load(\"ask_{0}_low.npy\".format(day))\n",
    "    bid_low = np.load(\"bid_{0}_low.npy\".format(day))\n",
    "\n",
    "    # Collect the spectral activity\n",
    "    mean_log_freq_delta_depth = mean_log_frequency(fourier_over_time_overlap_log(flux_low, 360000, 1000))\n",
    "    np.save(\"mlf_delta_depth_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_delta_depth)\n",
    "\n",
    "    mean_log_freq_depth = mean_log_frequency(fourier_over_time_overlap_log(flux_d, 360000, 1000))\n",
    "    np.save(\"mlf_depth_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_depth)\n",
    "\n",
    "    mean_log_freq_message = mean_log_frequency(fourier_over_time_overlap_log(flux_m, 360000, 1000))\n",
    "    np.save(\"mlf_message_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_message)\n",
    "\n",
    "    mean_log_freq_bid = mean_log_frequency(fourier_over_time_overlap_log(bid_low, 360000, 1000))\n",
    "    np.save(\"mlf_bid_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_bid)\n",
    "\n",
    "    mean_log_freq_ask = mean_log_frequency(fourier_over_time_overlap_log(ask_low, 360000, 1000))\n",
    "    np.save(\"mlf_ask_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_ask)\n",
    "\n",
    "\n",
    "    #Collect the mean\n",
    "    flux_dd_steps = get_mean_in_steps(flux_low, 360000, 1000)\n",
    "    np.save(\"avg_delta_depth_{0}_low_hour_window_step1000.npy\".format(day), flux_dd_steps)\n",
    "\n",
    "    flux_d_steps = get_mean_in_steps(flux_d, 360000, 1000)\n",
    "    np.save(\"avg_depth_{0}_low_hour_window_step1000.npy\".format(day), flux_d_steps)\n",
    "\n",
    "    flux_m_steps = get_mean_in_steps(flux_m, 360000, 1000)\n",
    "    np.save(\"avg_message_{0}_low_hour_window_step1000.npy\".format(day), flux_m_steps)\n",
    "\n",
    "    flux_ask_steps = get_mean_in_steps(ask_low, 360000, 1000)\n",
    "    np.save(\"avg_ask_{0}_low_hour_window_step1000.npy\".format(day), flux_ask_steps)\n",
    "\n",
    "    flux_bid_steps = get_mean_in_steps(bid_low, 360000, 1000)\n",
    "    np.save(\"avg_bid_{0}_low_hour_window_step1000.npy\".format(day), flux_bid_steps)\n",
    "\n",
    "\n",
    "    # We now collect the variance as a new feature.\n",
    "    message_var_steps = get_var_in_steps(flux_m, 360000, 1000)\n",
    "    np.save(\"message_var_{0}_low_hour_window_step1000.npy\".format(day), message_var_steps)\n",
    "\n",
    "    delta_var_steps = get_var_in_steps(flux_d, 360000, 1000)\n",
    "    np.save(\"delta_var_{0}_low_hour_window_step1000.npy\".format(day), delta_var_steps)\n",
    "\n",
    "    dd_var_steps = get_var_in_steps(flux_low, 360000, 1000)\n",
    "    np.save(\"delta_depth_var_{0}_low_hour_window_step1000.npy\".format(day), dd_var_steps)\n",
    "\n",
    "    ask_var_steps = get_var_in_steps(ask_low, 360000, 1000)\n",
    "    np.save(\"ask_var_{0}_low_hour_window_step1000.npy\".format(day), ask_var_steps)\n",
    "\n",
    "    bid_var_steps = get_var_in_steps(bid_low, 360000, 1000)\n",
    "    np.save(\"bid_var_{0}_low_hour_window_step1000.npy\".format(day), bid_var_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smaller_stocks = ['IGZ', 'IBO', 'CBX', 'CBO', 'ZBZX', 'MTFB', 'FMAX', 'ZVZZC', 'ZXYZ.A', \n",
    "                  'EVLMC', 'EVGBC', 'EVSTC', 'ZNWAA', 'GJO', 'SGYPU', 'TROVU', 'MFLA', \n",
    "                  'EMSA', 'EMLB', 'PFK', 'IGEM', 'SPE-B', 'OVLC', 'GDL-B', 'CCZ', 'OFG-D', \n",
    "                  'CUBI-D', 'GLU-A', 'SOV-C', 'HL-B', 'ITMS', 'ITML', 'BRG-C', 'CBMXW', \n",
    "                  'CUBI-E', 'CMS-B', 'NMK-C', 'GPT-A', 'MTB-', 'BCV-A', 'GJV', 'CVB', \n",
    "                  'GAB-D', 'XKE', 'HTF', 'STLR', 'GAB-G', 'KYN-F', 'ABRN', 'GGZ-A', 'WYIG', \n",
    "                  'DESC', 'PYT', 'PW-A', 'CPAA', 'ASB-C', 'ZAZZT', 'GCV-B', 'WBIR', 'PPS-A',\n",
    "                  'SUI-A', 'GAB-H', 'PCG-I']\n",
    "print(len(smaller_stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bid_low = np.zeros(int((16 - 9.5) * 3600 / 0.01))\n",
    "\n",
    "for stock in tqdm(lst):\n",
    "    stat = Statistic(stock)\n",
    "    if stat.obao is not None:\n",
    "        dd = stat.bid_flux_mat(d_t=0.01)\n",
    "        time, trace = dd[:, 0], dd[:, 1]\n",
    "        bid_low += trace\n",
    "\n",
    "\n",
    "np.save(\"bid_{0}_low.npy\".format(day), bid_low)\n",
    "\n",
    "ask_low = np.zeros(int((16 - 9.5) * 3600 / 0.01))\n",
    "\n",
    "for stock in tqdm(lst):\n",
    "    stat = Statistic(stock)\n",
    "    if stat.obao is not None:\n",
    "        dd = stat.ask_flux_mat(d_t=0.01)\n",
    "        time, trace = dd[:, 0], dd[:, 1]\n",
    "        ask_low += trace\n",
    "\n",
    "\n",
    "np.save(\"ask_{0}_low.npy\".format(day), ask_low)\n",
    "\n",
    "# start over from what we saved\n",
    "import numpy as np\n",
    "\n",
    "flux_low = np.load(\"delta_depth_flux_{0}_low.npy\".format(day))\n",
    "flux_d = np.load(\"depth_flux_{0}_low.npy\".format(day))\n",
    "flux_m = np.load(\"message_flux_{0}_low.npy\".format(day))\n",
    "ask_low = np.load(\"ask_{0}_low.npy\".format(day))\n",
    "bid_low = np.load(\"bid_{0}_low.npy\".format(day))\n",
    "\n",
    "# Collect the spectral activity\n",
    "mean_log_freq_delta_depth = mean_log_frequency(fourier_over_time_overlap_log(flux_low, 360000, 1000))\n",
    "np.save(\"mlf_delta_depth_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_delta_depth)\n",
    "\n",
    "mean_log_freq_depth = mean_log_frequency(fourier_over_time_overlap_log(flux_d, 360000, 1000))\n",
    "np.save(\"mlf_depth_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_depth)\n",
    "\n",
    "mean_log_freq_message = mean_log_frequency(fourier_over_time_overlap_log(flux_m, 360000, 1000))\n",
    "np.save(\"mlf_message_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_message)\n",
    "\n",
    "mean_log_freq_bid = mean_log_frequency(fourier_over_time_overlap_log(bid_low, 360000, 1000))\n",
    "np.save(\"mlf_bid_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_bid)\n",
    "\n",
    "mean_log_freq_ask = mean_log_frequency(fourier_over_time_overlap_log(ask_low, 360000, 1000))\n",
    "np.save(\"mlf_ask_{0}_low_hour_window_step1000.npy\".format(day), mean_log_freq_ask)\n",
    "\n",
    "\n",
    "#Collect the mean\n",
    "flux_dd_steps = get_mean_in_steps(flux_low, 360000, 1000)\n",
    "np.save(\"avg_delta_depth_{0}_low_hour_window_step1000.npy\".format(day), flux_dd_steps)\n",
    "\n",
    "flux_d_steps = get_mean_in_steps(flux_d, 360000, 1000)\n",
    "np.save(\"avg_depth_{0}_low_hour_window_step1000.npy\".format(day), flux_d_steps)\n",
    "\n",
    "flux_m_steps = get_mean_in_steps(flux_m, 360000, 1000)\n",
    "np.save(\"avg_message_{0}_low_hour_window_step1000.npy\".format(day), flux_m_steps)\n",
    "\n",
    "flux_ask_steps = get_mean_in_steps(ask_low, 360000, 1000)\n",
    "np.save(\"avg_ask_{0}_low_hour_window_step1000.npy\".format(day), flux_ask_steps)\n",
    "\n",
    "flux_bid_steps = get_mean_in_steps(bid_low, 360000, 1000)\n",
    "np.save(\"avg_bid_{0}_low_hour_window_step1000.npy\".format(day), flux_bid_steps)\n",
    "\n",
    "\n",
    "# We now collect the variance as a new feature.\n",
    "message_var_steps = get_var_in_steps(flux_m, 360000, 1000)\n",
    "np.save(\"message_var_{0}_low_hour_window_step1000.npy\".format(day), message_var_steps)\n",
    "\n",
    "delta_var_steps = get_var_in_steps(flux_d, 360000, 1000)\n",
    "np.save(\"delta_var_{0}_low_hour_window_step1000.npy\".format(day), delta_var_steps)\n",
    "\n",
    "dd_var_steps = get_var_in_steps(flux_low, 360000, 1000)\n",
    "np.save(\"delta_depth_var_{0}_low_hour_window_step1000.npy\".format(day), dd_var_steps)\n",
    "\n",
    "ask_var_steps = get_var_in_steps(ask_low, 360000, 1000)\n",
    "np.save(\"ask_var_{0}_low_hour_window_step1000.npy\".format(day), ask_var_steps)\n",
    "\n",
    "bid_var_steps = get_var_in_steps(bid_low, 360000, 1000)\n",
    "np.save(\"bid_var_{0}_low_hour_window_step1000.npy\".format(day), bid_var_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(time / 3600, ask_low)\n",
    "plt.plot(time / 3600, bid_low)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
